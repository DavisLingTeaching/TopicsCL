{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "A985Cem13abC",
        "dd30tgy2aW6s",
        "N6orA3HNgGaW"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Model Predictions (for English)\n",
        "\n",
        "Created Feburary 2023 by [Forrest Davis](https://conf.ling.cornell.edu/forrestdavis/). Get in touch if you have any questions!\n",
        "\n",
        "The following colab script will make concrete some of the issues about probability I discussed today and allow you to explore a bit on your own. If you've never used colab before, [here](https://colab.research.google.com) is a nice introductory document. It links to this, sort of unsettling, [video](https://www.youtube.com/watch?v=inN8seMm7UI).  \n",
        "\n",
        "What's critical for this notebook is running code. You can run code by hovering over \"code blocks\" and pressing the play button to the left. \n",
        "\n"
      ],
      "metadata": {
        "id": "Im8QmbyWncKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Push the play button to the left\n",
        "print('hello')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kehY8fgjoxO0",
        "outputId": "ed604ce5-1d4b-462d-9a51-0371c219dfcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up\n",
        "\n",
        "The code blocks in this section do the following: \n",
        "\n",
        "1. Install the necessary packages\n",
        "2. Import the libraries\n",
        "3. Clone a repo I made for evaluating models\n",
        "4. Move into the git repo"
      ],
      "metadata": {
        "id": "A985Cem13abC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "IAY7b5-hel-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aab8d16f-2c56-475b-f882-006814405b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 KB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.12.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, urllib3, multiprocess, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.9.0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.26.14 xxhash-3.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "import pandas as pd\n",
        "import sentencepiece"
      ],
      "metadata": {
        "id": "BHM5_lxmer0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31EOsl4Ddd-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f6a82b2-bc86-4e8e-9f9b-1286ebab6cec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PublicModelsAPI'...\n",
            "remote: Enumerating objects: 244, done.\u001b[K\n",
            "remote: Total 244 (delta 0), reused 0 (delta 0), pack-reused 244\u001b[K\n",
            "Receiving objects: 100% (244/244), 45.64 MiB | 28.17 MiB/s, done.\n",
            "Resolving deltas: 100% (119/119), done.\n",
            "/content/PublicModelsAPI\n"
          ]
        }
      ],
      "source": [
        "#Clone the evaluation repo\n",
        "!git clone https://github.com/forrestdavis/PublicModelsAPI.git\n",
        "# Move to evaluation repo for ease of running\n",
        "%cd /content/PublicModelsAPI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimenting with the model"
      ],
      "metadata": {
        "id": "EmPlUKlKHrHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I've provided three ways to query a neural model (the default being gpt2 small, linked [here](https://huggingface.co/gpt2) on huggingface): \n",
        "\n",
        "1. Interactive mode, where you enter sentences or phrases and incremental metrics are retreived from the model \n",
        "2. Targeted mode, where you have a fixed context and want to explore a set of possible continuations\n",
        "3. Completion mode, where you have a fixed context and want to know the top K next words (or subwords)"
      ],
      "metadata": {
        "id": "jLSyqVntG0PT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive Mode"
      ],
      "metadata": {
        "id": "ru67DoeP0ILj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code runs interactive mode with an English neural model (gpt2 small). The key columns are prob which gives you the probability assigned to that word in the input and surp which gives you the surprisal assigned to that word in the input. A video of running the code can be found [here](https://github.com/forrestdavis/PublicModelsAPI/blob/main/demo/Interactive.gif). The video is for slightly different code, but the mechanics are the same."
      ],
      "metadata": {
        "id": "WfJF7fQq0EA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following block of support code."
      ],
      "metadata": {
        "id": "UL2mp99FztZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Interactive code\n",
        "\n",
        "def getInteract(modelType='gpt2', modelName='gpt2'):\n",
        "\n",
        "    # set path\n",
        "    import sys\n",
        "    sys.path.append(\"/content/PublicModelsAPI/\")\n",
        "    from src.experiments.Interact import Interact\n",
        "\n",
        "    config = {\"exp\": \"Interact\", \n",
        "            \"models\": {modelType: [modelName]}, \n",
        "            \"lower\": False, \n",
        "            \"include_punct\": False\n",
        "            }\n",
        "    exp = Interact(config)\n",
        "    exp.run_interact()"
      ],
      "metadata": {
        "id": "-wLDXcSawfsy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getInteract()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "Rwg0qqCTXa8A",
        "outputId": "37d8b1d3-f0cb-42b6-9ded-d24e989c1a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running on cpu\n",
            "Using pad_token, but it is not set yet.\n",
            "Pad token was set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "string: The man who is tall is happy\n",
            "word                 | Split | Unk | Punct | ModelName            | surp     | prob      \n",
            "-----------------------------------------------------------------------------------------\n",
            "The                  |     0 |   0 |     0 | gpt2                 |        0 |          1\n",
            "man                  |     0 |   0 |     0 | gpt2                 |   10.116 |     0.0009\n",
            "who                  |     0 |   0 |     0 | gpt2                 |    3.143 |    0.11321\n",
            "is                   |     0 |   0 |     0 | gpt2                 |    5.448 |    0.02291\n",
            "tall                 |     0 |   0 |     0 | gpt2                 |   14.033 |      6e-05\n",
            "is                   |     0 |   0 |     0 | gpt2                 |     4.86 |    0.03442\n",
            "happy                |     0 |   0 |     0 | gpt2                 |   11.345 |    0.00038\n",
            "string: The man who is tall are happy\n",
            "word                 | Split | Unk | Punct | ModelName            | surp     | prob      \n",
            "-----------------------------------------------------------------------------------------\n",
            "The                  |     0 |   0 |     0 | gpt2                 |        0 |          1\n",
            "man                  |     0 |   0 |     0 | gpt2                 |   10.116 |     0.0009\n",
            "who                  |     0 |   0 |     0 | gpt2                 |    3.143 |    0.11321\n",
            "is                   |     0 |   0 |     0 | gpt2                 |    5.448 |    0.02291\n",
            "tall                 |     0 |   0 |     0 | gpt2                 |   14.033 |      6e-05\n",
            "are                  |     0 |   0 |     0 | gpt2                 |   12.787 |    0.00014\n",
            "happy                |     0 |   0 |     0 | gpt2                 |   11.636 |    0.00031\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-16ba3092860b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetInteract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-54-05b5e1067e09>\u001b[0m in \u001b[0;36mgetInteract\u001b[0;34m(modelType, modelName)\u001b[0m\n\u001b[1;32m     14\u001b[0m             }\n\u001b[1;32m     15\u001b[0m     \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInteract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_interact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/PublicModelsAPI/src/experiments/Interact.py\u001b[0m in \u001b[0;36mrun_interact\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'string: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Targeted Mode\n",
        "\n",
        "In targeted mode, you provide a context string and a set of target words. The probability of these words is returned to you. First you'll need to run the following block of code which sets up the relevant helper functions (this only needs to be run once). Then just change the context and target variables' values as you desire and run the code block. "
      ],
      "metadata": {
        "id": "7d-VjhAuILGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Targeted code\n",
        "def getTargeted(context, targets, \n",
        "                modelType='gpt2', modelName='gpt2'):\n",
        "    \n",
        "    # set path\n",
        "    import sys\n",
        "    sys.path.append(\"/content/PublicModelsAPI/\")\n",
        "    from src.models import models\n",
        "\n",
        "    run_config = {'models': {modelType: [modelName]}}\n",
        "\n",
        "    LM = models.load_models(run_config)[0]\n",
        "\n",
        "    Ps = []\n",
        "    for target in targets:\n",
        "        sent = context.strip() + ' '+target.strip()\n",
        "        #Get likelihood of final word\n",
        "        prob = LM.get_aligned_words_probabilities(sent)[0][-1]\n",
        "        assert prob.word == target\n",
        "        Ps.append((target, prob.prob))\n",
        "\n",
        "    return Ps"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mb7OFU7sPfNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change these variables\n",
        "context = 'I saw a fragile'\n",
        "targets = ['of', 'whale']\n",
        "\n",
        "# Leave this as is \n",
        "probs = getTargeted(context, targets)\n",
        "print('----------------------------------')\n",
        "for p in probs:\n",
        "    print(f\"P({p[0]}|{context}) = {round(p[1], 6)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2-raEm7TBkA",
        "outputId": "8dfab178-7d79-4865-bcee-c65303533b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running on cpu\n",
            "Using pad_token, but it is not set yet.\n",
            "Pad token was set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------\n",
            "P(of|I saw a fragile) = 0.000334\n",
            "P(whale|I saw a fragile) = 0.000169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Completion Mode\n",
        "\n",
        "In completion mode, the top K next word (or subword) predictions are returned to you. Run the following code block once to set up the relevant code. Then, the final code block is used to set your relevant context value and the number of completions you want to see. That final block should be run in order to generate the results."
      ],
      "metadata": {
        "id": "HwqDuMHGT3gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Completion code\n",
        "def getTopK(context, k=10, \n",
        "            modelType='gpt2', modelName='gpt2'):\n",
        "    \n",
        "    # set path\n",
        "    import sys\n",
        "    import torch\n",
        "    sys.path.append(\"/content/PublicModelsAPI/\")\n",
        "    from src.models import models\n",
        "\n",
        "    run_config = {'models': {modelType: [modelName]}}\n",
        "\n",
        "    LM = models.load_models(run_config)[0] \n",
        "\n",
        "    output = LM.get_output(context)\n",
        "    logits = output[-1]\n",
        "    #Final predictions\n",
        "    final = torch.nn.functional.softmax(logits[0,-1,:], dim=-1)\n",
        "    #Get topk predictions\n",
        "    topK = torch.topk(final, k)\n",
        "    values = topK.values.tolist()\n",
        "    indices = topK.indices\n",
        "    #Convert to token representations\n",
        "    tokens = LM.tokenizer.convert_ids_to_tokens(indices)\n",
        "    #Strip off byte\n",
        "    tokens = list(map(lambda x: x.replace('Ġ', ''), tokens))\n",
        "    #Safety check\n",
        "    assert len(tokens) == len(values)\n",
        "\n",
        "    return list(zip(tokens, values))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RoQvi13lUroO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = 'Noam Chomsky is a'\n",
        "K = 10\n",
        "\n",
        "predictions = getTopK(context, K)\n",
        "\n",
        "print(f\"Context: {context}\")\n",
        "print(\"\\tword\"+\" \"*16+\"prob\")\n",
        "for pred in predictions:\n",
        "    print(f\"\\t{pred[0]: <20}{round(pred[1], 6)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1Xqm7waW-xS",
        "outputId": "304b8158-1bdf-4224-9d75-607719404f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running on cpu\n",
            "Using pad_token, but it is not set yet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: Noam Chomsky is a\n",
            "\tword                prob\n",
            "\tprofessor           0.107418\n",
            "\tformer              0.044045\n",
            "\twriter              0.021567\n",
            "\tjournalist          0.020247\n",
            "\tlingu               0.018673\n",
            "\tsenior              0.014657\n",
            "\thistorian           0.013433\n",
            "\tphilosopher         0.013281\n",
            "\tpolitical           0.012487\n",
            "\tscholar             0.012242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pad token was set\n"
          ]
        }
      ]
    }
  ]
}